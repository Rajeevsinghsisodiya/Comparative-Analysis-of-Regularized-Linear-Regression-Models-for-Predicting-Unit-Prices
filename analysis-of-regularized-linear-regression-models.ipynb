{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9340284,"sourceType":"datasetVersion","datasetId":5660418}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-07T16:36:21.774553Z","iopub.execute_input":"2024-09-07T16:36:21.775817Z","iopub.status.idle":"2024-09-07T16:36:21.806318Z","shell.execute_reply.started":"2024-09-07T16:36:21.775738Z","shell.execute_reply":"2024-09-07T16:36:21.804702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Project Title: \"Comparative Analysis of Regularized Linear Regression Models for Predicting Unit Prices\"**\n\nContributor: Rajeev Singh sisodiya\n\nProject Overview:\n\nThis project aims to predict the Unit Price of products in a dataset using various linear regression models with different regularization techniques. The key objective is to evaluate the performance of Unregularized Linear Regression, L1 (Lasso), L2 (Ridge), and Elastic Net models, and identify the best approach for this specific prediction task.\n\nData Preparation:\n\nThe dataset contains features such as Unit Price, Quantity, Tax (5%), Total, Cost of Goods Sold (COGS), Gross Margin Percentage, Gross Income, and Rating. The Invoice ID column was excluded since it is non-numeric and not relevant for scaling or prediction. The training and test datasets were split, with both sets containing the same selected features.\n\nFeature Scaling:\n\nThe features were standardized using StandardScaler to ensure that the different models can perform optimally, especially since regularization techniques (Lasso, Ridge, and Elastic Net) are sensitive to the scale of the input data.\n\nModel Selection: Four models were used for comparison\n\nUnregularized Linear Regression: A basic linear model without any regularization.\n\nL1 (Lasso) Regression: Applies L1 regularization to reduce overfitting by driving some coefficients to zero, helping with feature selection.\n\nL2 (Ridge) Regression: Uses L2 regularization, reducing the size of the coefficients to avoid overfitting.\n\nElastic Net: A combination of both L1 and L2 penalties to balance between feature selection and shrinkage.\n\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np \nimport seaborn as sns\nimport matplotlib.pyplot as plt \n%matplotlib inline \n\nimport warnings \nwarnings.filterwarnings(\"ignore\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-07T16:36:40.363950Z","iopub.execute_input":"2024-09-07T16:36:40.365551Z","iopub.status.idle":"2024-09-07T16:36:40.375135Z","shell.execute_reply.started":"2024-09-07T16:36:40.365474Z","shell.execute_reply":"2024-09-07T16:36:40.373045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv(\"/kaggle/input/super-market-data/supermarket_sales - Sheet1.csv\")\ndf_test = pd.read_csv(\"/kaggle/input/super-market-data/supermarket_sales - Sheet1.csv\")\nprint(\"Training Data: \\n\")\ndisplay(df_train.head(2))\nprint(\"Testing Data: \\n\")\ndf_test.head(2)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-07T16:36:46.899918Z","iopub.execute_input":"2024-09-07T16:36:46.900476Z","iopub.status.idle":"2024-09-07T16:36:46.963593Z","shell.execute_reply.started":"2024-09-07T16:36:46.900424Z","shell.execute_reply":"2024-09-07T16:36:46.962375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.shape\n","metadata":{"execution":{"iopub.status.busy":"2024-09-07T16:36:53.611461Z","iopub.execute_input":"2024-09-07T16:36:53.612021Z","iopub.status.idle":"2024-09-07T16:36:53.620328Z","shell.execute_reply.started":"2024-09-07T16:36:53.611971Z","shell.execute_reply":"2024-09-07T16:36:53.618949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\n\n# Exclude the 'Invoice ID' column as it's non-numeric and not suitable for scaling\nX_train = df_train[['Unit price', 'Quantity', 'Tax 5%', 'Total', 'cogs', 'gross margin percentage', 'gross income', 'Rating']] # Removed non-numeric columns\n\ny_train = df_train['Unit price']\n\nX_test = df_test[['Unit price', 'Quantity', 'Tax 5%', 'Total', 'cogs', 'gross margin percentage', 'gross income', 'Rating']] # Removed non-numeric columns\n\ny_test = df_test['Unit price']\n\n# Scale the features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nmodels = {\n    'Unregularized': LinearRegression(),\n    'L1 (Lasso)': Lasso(alpha=1.0),\n    'L2 (Ridge)': Ridge(alpha=1.0),\n    'Elastic Net': ElasticNet(alpha=1.0, l1_ratio=0.5)  \n}\n\nresults = {}\n\nfor name, model in models.items():\n    model.fit(X_train_scaled, y_train)\n    y_pred = model.predict(X_test_scaled)\n    mse = mean_squared_error(y_test, y_pred)\n    r2 = model.score(X_test_scaled, y_test)\n    results[name] = {'model': model, 'mse': mse, 'r2': r2}\n\nprint(\"Model Results:\")\ncolor_map = plt.cm.tab20  \ncolors = color_map(np.arange(len(results)) % color_map.N)  \n\nfor i, (name, result) in enumerate(results.items()):\n    print(f\"\\033[1m{name}:\\033[0m\")  \n    print(f\"  MSE: {result['mse']:.4f}\")\n    print(f\"  R2 Score: {result['r2']:.4f}\")\n    print(f\"  Coefficients:\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-07T16:36:58.574011Z","iopub.execute_input":"2024-09-07T16:36:58.574549Z","iopub.status.idle":"2024-09-07T16:36:58.617572Z","shell.execute_reply.started":"2024-09-07T16:36:58.574499Z","shell.execute_reply":"2024-09-07T16:36:58.616215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Key Observations:**\n\nUnregularized Model:\n\nMSE: 0.0000\n\nR² Score: 1.0000\n\nThis model has a perfect fit on the training data, with an MSE of 0 and R² of 1. However, the lack of regularization might lead to overfitting, as it could be too closely fitting the training data, which could harm its generalization to unseen data.\n\nL1 (Lasso) Regularization:\n\nMSE: 1.0000\n\nR² Score: 0.9986\n\nLasso regularization introduces some shrinkage in the model, reducing the magnitude of coefficients and potentially setting some to zero. The MSE slightly increases, and the R² slightly decreases, indicating some trade-off for sparsity and improved generalization, but the model is still highly accurate.\n\nL2 (Ridge) Regularization:\n\nMSE: 0.0032\n\nR² Score: 1.0000\n\nRidge regularization also applies a penalty to the coefficients, but unlike Lasso, it reduces them without setting them to zero. The model achieves a very low MSE and maintains a high R² score, indicating excellent performance with some regularization applied to prevent overfitting.\n\nElastic Net Regularization:\n\nMSE: 68.0373\n\nR² Score: 0.9030\n\nElastic Net is a combination of both L1 and L2 penalties. However, in this case, the model performance is significantly worse, with a high MSE and a much lower R² score. This suggests that the combination of both penalties might be too harsh for this dataset or that the balance between L1 and L2 penalties needs to be adjusted.\n\nSummary: The unregularized model performs perfectly on training data, but regularization improves generalization.\n\nRidge regularization (L2) performs the best in terms of reducing overfitting while maintaining model accuracy.\n\nLasso (L1) is also strong but introduces sparsity, which may help in feature selection.\n\nElastic Net seems to need some tuning, as it currently leads to the worst performance among the models.\n\nAdjustments to hyperparameters, such as the penalty strength in Lasso and Elastic Net, could help improve the model further.","metadata":{}},{"cell_type":"markdown","source":"Note: By carefully considering these factors, we can make an informed decision about the best model for our specific problem.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(24, 10))\n\nx = np.arange(len(X_train.columns))  \nwidth = 0.20  # Bar width\n\ncmap = plt.cm.get_cmap('tab20')  \ncolors = cmap(np.arange(len(results)) % cmap.N)  \n\nfor i, (name, result) in enumerate(results.items()):\n    coef = result['model'].coef_.ravel() \n    \n    bars = plt.bar(x + i * width, coef, width, label=name, color=colors[i])\n    \n    for bar in bars:\n        height = bar.get_height()\n        plt.text(bar.get_x() + bar.get_width()/2, 0,\n                 f\"{height:.4f}\", ha='center', va='baseline', \n                 rotation=90, fontsize=18, color='Black',\n                 bbox=dict(facecolor='white', edgecolor='none', alpha=0.7)) \n\nplt.axhline(y=0, color='k', linestyle='-', linewidth= 1) \nplt.xlabel('Features', fontsize=20, color='Blue', fontweight='bold')\nplt.ylabel('Coefficient Value', fontsize=20, color='Blue', fontweight='bold')\nplt.title('Feature Importance Comparison', fontsize=24, color='Blue', fontweight='bold', pad=20)\nplt.xticks(x + width, X_train.columns, fontsize=16, color='Blue', rotation=45, ha='right') \nplt.legend(fontsize=16)\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-09-07T16:37:05.696324Z","iopub.execute_input":"2024-09-07T16:37:05.696952Z","iopub.status.idle":"2024-09-07T16:37:06.630890Z","shell.execute_reply.started":"2024-09-07T16:37:05.696895Z","shell.execute_reply":"2024-09-07T16:37:06.629240Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot predictions vs actual for each model\nfig, axes = plt.subplots(1, 4, figsize=(24, 8))\nfig.suptitle('Predictions vs Actual', fontsize=26, color='#8B4513', fontweight='bold')\n\nfor ax, (name, result) in zip(axes, results.items()):\n    model = result['model']\n    y_pred = model.predict(X_test_scaled)\n    ax.scatter(y_test, y_pred, alpha=0.5)\n    ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n    ax.set_xlabel('Actual', fontsize=16, color='Blue', fontweight='bold')\n    ax.set_ylabel('Predicted', fontsize=16, color='Blue', fontweight='bold')\n    ax.set_title(f'{name}\\nMSE: {result[\"mse\"]:.4f}, R2: {result[\"r2\"]:.4f}', fontsize=18, color='Blue', fontweight='bold')\n    ax.tick_params(axis='both', which='major', labelsize=12)\n\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-09-07T16:41:37.072981Z","iopub.execute_input":"2024-09-07T16:41:37.073526Z","iopub.status.idle":"2024-09-07T16:41:38.236934Z","shell.execute_reply.started":"2024-09-07T16:41:37.073465Z","shell.execute_reply":"2024-09-07T16:41:38.235638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Key Observations from the Plot Results Code:**\n\nThe model with the scatter points closest to the red dashed line and the lowest MSE/highest R² is likely the best performing.\n\nWide scatter or significant deviations from the line indicate models with poorer predictions, possibly due to underfitting or overly strong regularization.\n\nThis visualization gives both a qualitative (visual) and quantitative (MSE/R²) assessment of each model's ability to predict accurately.\n\nThis kind of side-by-side comparison helps quickly understand which model generalizes well to unseen data and where improvements might be needed.","metadata":{}},{"cell_type":"markdown","source":"**Model Evaluation:**\n\nEach model was trained on the scaled training dataset and then evaluated using the test dataset. Mean Squared Error (MSE) and R² Score were calculated to assess the performance of each model. MSE indicates how close the predicted values are to the actual values, with lower values being better. R² Score shows the proportion of variance explained by the model, with scores closer to 1 indicating a better fit.\n\n","metadata":{}}]}